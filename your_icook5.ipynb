{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "{'User-agent': 'Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)'}{'User-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36 OPR/38.0.2220.41'}\n",
      "\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "https://icook.twhttps://icook.tw/categories/612?page=1\n",
      "{'User-agent': ' Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
      "https://icook.twhttps://icook.tw/categories/206?page=1\n",
      "{'User-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36 OPR/38.0.2220.41'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 157, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\util\\connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\socket.py\", line 752, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11004] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 376, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 994, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 300, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 169, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F3C5388>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='icook.twhttps', port=443): Max retries exceeded with url: //icook.tw/categories/612?page=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F3C5388>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-540f4ef4c625>\", line 142, in job\n",
      "    get_soup(url,headers)\n",
      "  File \"<ipython-input-3-540f4ef4c625>\", line 24, in get_soup\n",
      "    res=ss.get(url,headers=headers)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 543, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='icook.twhttps', port=443): Max retries exceeded with url: //icook.tw/categories/612?page=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F3C5388>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed'))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multithread took 5.656000137329102 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 157, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\util\\connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\socket.py\", line 752, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11004] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 376, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 994, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 300, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connection.py\", line 169, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F6DD1C8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='icook.twhttps', port=443): Max retries exceeded with url: //icook.tw/categories/206?page=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F6DD1C8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-540f4ef4c625>\", line 142, in job\n",
      "    get_soup(url,headers)\n",
      "  File \"<ipython-input-3-540f4ef4c625>\", line 24, in get_soup\n",
      "    res=ss.get(url,headers=headers)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 543, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"E:\\python\\anaconda3\\envs\\python_study\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='icook.twhttps', port=443): Max retries exceeded with url: //icook.tw/categories/206?page=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F6DD1C8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import emoji\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#流程控制\n",
    "def random_headers():\n",
    "    headers_list = [{'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'},{'User-agent':' Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'},{'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36 OPR/38.0.2220.41'},{'User-agent':'Mozilla/5.0 (Linux; U; Android 4.0.3; de-ch; HTC Sensation Build/IML74K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'},{'User-agent':'Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)'}]\n",
    "    headers=random.choice(headers_list)\n",
    "    print(headers)\n",
    "    return headers\n",
    "    \n",
    "\n",
    "    \n",
    "def get_soup(url,headers):\n",
    "    proxy_list=[{'https':'59.124.224.180:3128','http':'59.124.224.180:3128'},{'https':'113.196.140.162:8888','http':'113.196.140.162:8888'}]\n",
    "    proxy_list2={'http':'http://220.135.165.38:8080','https':'220.135.168.38:8080'}\n",
    "\n",
    "    ss=requests.session()\n",
    "    res=ss.get(url,headers=headers)\n",
    "  \n",
    "    \n",
    "    res_status_code=res.status_code\n",
    "    print(res_status_code)\n",
    "    \n",
    "    if res_status_code!=200 :\n",
    "        time.sleep(2)\n",
    "        print(\"進入proxy狀態\")\n",
    "        proxy=random.choice(proxy_list)\n",
    "        change_count+=1\n",
    "        res = requests.get(url ,proxies=proxy ,headers=headers)\n",
    "        soup=BeautifulSoup(res.text,'html.parser')\n",
    "   \n",
    "    soup=BeautifulSoup(res.text,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def preprocess (x):\n",
    "    x = emoji.get_emoji_regexp().sub(r'', x)\n",
    "    x = re.sub(r'[［(（【{][^】)}）］]*[】)}）］]', '', x)\n",
    "    x = x.strip().replace(\" \", \"\")\n",
    "    x = re.sub(r'[~\\-。]', \"\", x) \n",
    "    return x\n",
    "\n",
    "    \n",
    "\n",
    "#random_headers()\n",
    "\n",
    "\n",
    "#初始變數\n",
    "\n",
    "headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'}\n",
    "url = 'https://icook.tw/categories?ref=icook-footer'\n",
    "get_soup(url,headers)\n",
    "soup=get_soup(url,headers)\n",
    "\n",
    "#tag_list = soup.select('a.categories-all-child-link')\n",
    "#section_ls = []\n",
    "#for tag in tag_list:\n",
    "#    section_ls.append(tag['href'])\n",
    "\n",
    "#your_section = section_ls[len(section_ls)//2:]\n",
    "\n",
    "#your_section=['/categories/39' , '/categories/64', '/categories/63', '/categories/216', '/categories/350', '/categories/219', '/categories/20', '/categories/463', '/categories/599', '/categories/462', '/categories/148', '/categories/27', '/categories/147', '/categories/210', '/categories/206', '/categories/77', '/categories/498', '/categories/453', '/categories/495', '/categories/29', '/categories/30', '/categories/455', '/categories/456', '/categories/13', '/categories/449', '/categories/26', '/categories/25', '/categories/110', '/categories/417', '/categories/458', '/categories/459', '/categories/600', '/categories/612', '/categories/613', '/categories/614', '/categories/616', '/categories/211', '/categories/185', '/categories/52', '/categories/71', '/categories/137', '/categories/49', '/categories/50', '/categories/602', '/categories/607']\n",
    "#your_section=['/categories/600', '/categories/614', '/categories/417', '/categories/25', '/categories/456', '/categories/459', '/categories/613', '/categories/219', '/categories/185', '/categories/607', '/categories/30', '/categories/350', '/categories/498', '/categories/616', '/categories/52', '/categories/64', '/categories/462', '/categories/599', '/categories/137', '/categories/63', '/categories/210', '/categories/211', '/categories/458', '/categories/49', '/categories/71', '/categories/453', '/categories/13', '/categories/110', '/categories/602', '/categories/77', '/categories/20', '/categories/463', '/categories/455', '/categories/495', '/categories/216', '/categories/449', '/categories/148', '/categories/50', '/categories/26', '/categories/29', '/categories/147', '/categories/206', '/categories/39', '/categories/612', '/categories/27']\n",
    "\n",
    "#your_section1=your_section[0:9]\n",
    "your_section1=['/categories/49', '/categories/71','/categories/453','/categories/458']\n",
    "\n",
    "\n",
    "\n",
    "def job(x):\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "    domain = 'https://icook.tw'\n",
    "    url_categories = x\n",
    "    page = 1\n",
    "    vip_count=0\n",
    "    c = 0\n",
    "    url = domain + url_categories\n",
    "    df = pd.DataFrame(columns = ['Recipe Name','Author','Recipe URL','Recipe Picture','Servings','Time','Ingredients',\\\n",
    "                                 'Unit','Recipe Like','Publish Date','Views Number','Recipe Steps'])\n",
    "    headers=random_headers()\n",
    "    soup=get_soup(url,headers)\n",
    "    section_number = url.split('/')[2]\n",
    "    total_amount = soup.select('title')[0].text\n",
    "    total_amount = int(''.join(re.findall(\"\\d+\", total_amount)))\n",
    "    max_page = 0\n",
    "    if total_amount % 18 == 0:\n",
    "        max_page = total_amount // 18\n",
    "    else:\n",
    "        max_page = (total_amount // 18) + 1\n",
    "    \n",
    "    while(page <= max_page):\n",
    "        page_index = f'?page={page}'\n",
    "        url = domain + url + page_index\n",
    "        print(url)\n",
    "        headers= random_headers()\n",
    "        soup=get_soup(url,headers)\n",
    "        food_list = soup.select('a.browse-recipe-touch-link')\n",
    "        for food in food_list:\n",
    "            headers= random_headers()\n",
    "            soup=get_soup(url,headers)\n",
    "            \n",
    "            #recipe_section = soup_recipe.select('a.recipe-breadcrumb-item-link')[1].text.strip()\n",
    "            #recipe_title = \"\".join((soup_recipe.select('h1.title')[0].text).split())\n",
    "            #recipe_title = preprocess(recipe_title)\n",
    "            recipe_title = soup.select('h1.title')[0].text\n",
    "            recipe_title = preprocess(recipe_title)\n",
    "            check_vip=soup.select('h1.title')[0].select('span.badge-vip')\n",
    "            if len(check_vip)<=0:\n",
    "                pass\n",
    "                #print(\"is not vip data, data will be save\")\n",
    "            else:\n",
    "                print(\"this is vip data,data will not save\")\n",
    "                vip_count+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            recipe_author = soup.select('a.author-name-link')[0].text\n",
    "            recipe_pic_url = soup.select(\"img.main-pic\")[0]['src']\n",
    "            \n",
    "        \n",
    "            if len(soup.select('span.stat-content')) == 2:\n",
    "                recipe_like = 0\n",
    "            else:\n",
    "                if '萬' in soup.select('span.stat-content')[0].text:\n",
    "                    recipe_like = int(soup.select('span.stat-content')[0].text[:-5].replace('.','')+\"000\")\n",
    "                else:\n",
    "                    recipe_like = int(soup.select('span.stat-content')[0].text[:-3].replace(',',''))\n",
    "        \n",
    "            if not soup.select('div.servings-info.info-block') and not soup.select('div.time-info.info-block'):\n",
    "                recipe_servings = 0\n",
    "                recipe_time = 0\n",
    "            elif not soup.select('div.time-info.info-block'):\n",
    "                recipe_servings = int(soup.select('span.num')[0].text)\n",
    "                recipe_time = 0\n",
    "            elif  not soup.select('div.servings-info.info-block'): \n",
    "                recipe_servings = 0\n",
    "                recipe_time = int(soup.select('span.num')[0].text)\n",
    "            else: \n",
    "                recipe_servings = int(soup.select('span.num')[0].text)\n",
    "                recipe_time = int(soup.select('span.num')[1].text)\n",
    "\n",
    "            recipe_ingredient = \"\"\n",
    "            for ingredient in soup.select('.ingredient-search'):\n",
    "                if ingredient.text == \"請參考圖片說明\":\n",
    "                    continue\n",
    "                else:\n",
    "                    recipe_ingredient += ingredient.text + \" \" \n",
    "            recipe_ingredient = emoji.get_emoji_regexp().sub(r'', recipe_ingredient)\n",
    "        \n",
    "            recipe_unit = \"\"\n",
    "            for unit in soup.select('div.ingredient-unit'):\n",
    "                if unit.text == \"内文有\":\n",
    "                    continue\n",
    "                else:\n",
    "                    recipe_unit += unit.text + \" \"\n",
    "\n",
    "            recipe_steps = \"\"\n",
    "            counter = 1\n",
    "            for steps in soup.select('div.step-instruction-content'):\n",
    "                recipe_steps += str(counter) + \" \" + steps.text + \"\\n\"\n",
    "                recipe_steps = emoji.get_emoji_regexp().sub(r'', recipe_steps)\n",
    "                counter += 1\n",
    "            publish_date = soup.select('span.meta-content')\n",
    "            if len (publish_date)<=0:\n",
    "                print(\"publish_date is na\")\n",
    "                publish_date=\"na\"\n",
    "            else:\n",
    "                publish_date = publish_date[0].text[:-3]\n",
    "        \n",
    "            views = soup.select('span.meta-content')\n",
    "            if len(views)<=0:\n",
    "                print(\"view is na\")\n",
    "                views_number=0\n",
    "                views=\"0\"\n",
    "            else:\n",
    "                views=views[1].text[:-3]\n",
    "                if '萬' in views:\n",
    "                    views_number = views[:-2]\n",
    "                    views_number = int(views_number.replace('.','')+\"000\")\n",
    "                else:\n",
    "                    views_number = int(views.replace(',',''))\n",
    "        \n",
    "        \n",
    "            \n",
    "            #print(recipe_section)    \n",
    "            print(recipe_url)\n",
    "            print(recipe_title)\n",
    "            #print(recipe_author)\n",
    "            #print(recipe_pic_url)\n",
    "            #print(recipe_like)\n",
    "            #print(recipe_servings)\n",
    "            #print(recipe_time)\n",
    "            #print(recipe_ingredient)\n",
    "            #print(recipe_unit)\n",
    "            #print(recipe_steps)\n",
    "            #print(publish_date)\n",
    "            #print(views)\n",
    "            print(\"----\")\n",
    "        \n",
    "            ls = [recipe_title,recipe_author,recipe_url,recipe_pic_url,recipe_servings,recipe_time,recipe_ingredient,\\\n",
    "                  recipe_unit,recipe_like,publish_date,views_number,recipe_steps]\n",
    "            df.loc[c] = ls\n",
    "            c += 1\n",
    "    \n",
    "            time.sleep(random.randint(1, 4))\n",
    "        \n",
    "        \n",
    "    \n",
    "        page += 1\n",
    "    df.to_csv(f'./{section_number}.csv', index=False ,encoding='utf-8-sig')\n",
    "    print(\"有多少{}vip資料\".format(vip_count))\n",
    "\n",
    "\n",
    "threads = list()\n",
    "starttime = time.time()\n",
    "#a = ['/categories/347','/categories/346']\n",
    "for element in your_section1:\n",
    "    threads.append(threading.Thread(target=job, args=(element,)))\n",
    "for thread in threads:    \n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print('Multithread took {} seconds'.format(time.time() - starttime))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_headers():\n",
    "    headers_list = [{'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'},{'User-agent':' Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'},{'User-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36 OPR/38.0.2220.41'},{'User-agent':'Mozilla/5.0 (Linux; U; Android 4.0.3; de-ch; HTC Sensation Build/IML74K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'},{'User-agent':'Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)'}]\n",
    "    headers=random.choice(headers_list)\n",
    "    print(headers)\n",
    "    return headers\n",
    "\n",
    "random_headers()\n",
    "\n",
    "\n",
    "#從首頁開始爬是https://cookpad.com/tw after 後參數為linux_time 在轉換為現在時間\n",
    "#while暫停條件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "['/categories/6', '/categories/301', '/categories/302', '/categories/3', '/categories/40', '/categories/38/categories/16', '/categories/18']\n",
      "['/categories/600', '/categories/614', '/categories/417', '/categories/25', '/categories/456', '/categories/459', '/categories/613', '/categories/219']\n",
      "['/categories/185', '/categories/607', '/categories/30', '/categories/350', '/categories/498', '/categories/616', '/categories/52', '/categories/64']\n",
      "['/categories/462', '/categories/599', '/categories/137', '/categories/63', '/categories/210', '/categories/211', '/categories/458', '/categories/49']\n",
      "['/categories/71', '/categories/453', '/categories/13', '/categories/110', '/categories/602', '/categories/77', '/categories/20', '/categories/463']\n",
      "['/categories/455', '/categories/495', '/categories/216', '/categories/449', '/categories/148', '/categories/50', '/categories/26', '/categories/29']\n",
      "['/categories/147', '/categories/206', '/categories/39', '/categories/612', '/categories/27']\n"
     ]
    }
   ],
   "source": [
    "print(len(your_section))\n",
    "print(your_section1)\n",
    "print(your_section[:8])\n",
    "print(your_section[8:16])\n",
    "print(your_section[16:24])\n",
    "print(your_section[24:32])\n",
    "print(your_section[32:40])\n",
    "print(your_section[40:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['/categories/6', '/categories/301', '/categories/302', '/categories/3', '/categories/40', '/categories/38''/categories/16', '/categories/18']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
